#!/usr/bin/env python3
"""
Statistical Testing Module
==========================

This module provides statistical testing utilities for the benchmarking experiments.
It includes per-pipeline permutation tests vs chance and paired pipeline comparisons
for cross-site evaluation.

Implemented tests
-----------------
1) Permutation test vs chance (pipeline vs chance)
   - Functions: permutation_test, permutation_test_precomputed,
     run_classification_with_permutation
   - Null: no association between features and labels (permute y_train)
   - Output: observed accuracy + null distribution + p-value

2) Paired randomization / sign-flip tests (paired samples or subjects)
   - Functions: paired_randomization_test, sign_flip_randomization_test,
     factor_level_randomization_test
   - Use case: planned factor-level comparisons (e.g., GSR vs noGSR) with
     matched pipelines, aggregated to subject-level losses

3) Exact McNemar test (A vs B accuracy on the same test set)
   - Function: mcnemar_exact_test
   - Input: y_true, y_pred_a, y_pred_b

4) DeLong test for correlated ROC-AUCs (A vs B)
   - Function: delong_roc_test
   - Input: y_true, score_a, score_b (same test samples)

5) Bootstrap CI (optional, effect size)
   - Function: bootstrap_mean_ci (used with factor-level tests)

Input files for pipeline comparisons
------------------------------------
Cross-site comparisons require per-test-sample outputs:
- `results/pipelines/<atlas>_strategy-<strategy>_<gsr>/cross_site_<train>2<test>_test_outputs.csv`
  generated by `benchmarking/ml/pipeline.py`.
  Required columns: train_site, test_site, atlas, fc_type, strategy, gsr,
  model, model_params, test_subject, y_true, y_pred, y_score, p_positive.
- `pipeline_abbreviations.csv` maps pipeline abbrev (P0001, ...) to
  full specs. Used by compare_pipelines when you compare by abbrev.

Output shape (helpers)
----------------------
- factor_level_randomization_test returns a dict with effect size (observed_delta),
  p-value, and counts (n_subjects, n_pairs).
- compare_pipelines returns dict with McNemar and DeLong results.

How to run (examples)
---------------------
Library usage from a notebook or script:

    import pandas as pd
    from benchmarking.ml import stats

    df = pd.read_csv(
        "results/pipelines/Schaefer200_strategy-1_GSR/cross_site_ihb2china_test_outputs.csv"
    )
    abbrev = pd.read_csv("results/pipelines/pipeline_abbreviations.csv")

    # Option 1: factor-level test (GSR vs noGSR), ihb -> china
    res = stats.factor_level_randomization_test(
        df, factor="gsr", level_a="GSR", level_b="noGSR",
        metric="log_loss", train_site="ihb", test_site="china",
        n_permutations=10000, n_bootstrap=10000,
    )

    # Option 3: pipeline A vs B by abbreviation
    res_ab = stats.compare_pipelines(df, "P0001", "P0002", abbrev_df=abbrev)

References
----------
- Ojala & Garriga (2010). Permutation Tests for Studying Classifier Performance.
- Combrisson & Jerbi (2015). Exceeding chance level by chance.
- DeLong, DeLong, & Clarke-Pearson (1988). Comparing correlated ROC-AUCs.

Author: BenchmarkingEOEC Team
"""

import json
import numpy as np
import pandas as pd
from typing import Optional, Dict, Any, Literal, Iterable
from itertools import combinations
from sklearn.base import clone
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, roc_auc_score, brier_score_loss
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from scipy import stats
from nilearn.connectome import sym_matrix_to_vec


ModelName = Literal["logreg", "svm_rbf"]
Alternative = Literal["two-sided", "greater", "less"]


def build_estimator(
    model: ModelName,
    *,
    random_state: int = 42,
    params: Optional[Dict[str, Any]] = None,
):
    params = dict(params or {})

    if model == "logreg":
        defaults: Dict[str, Any] = {"solver": "lbfgs", "max_iter": 1000, "random_state": random_state}
        return LogisticRegression(**{**defaults, **params})

    if model == "svm_rbf":
        defaults = {"kernel": "rbf", "C": 1.0, "gamma": "scale"}
        params.pop("kernel", None)
        params.pop("random_state", None)  # SVC doesn't support random_state unless probability=True
        return SVC(**{**defaults, **params})

    raise ValueError(f"Unknown model: {model}")


def preprocess_features(
    X_train: np.ndarray,
    X_test: np.ndarray,
    *,
    pca_components: Optional[float] = 0.95,
    random_state: int = 42,
    scale: bool = False,
    vectorize: bool = True,
) -> tuple[np.ndarray, np.ndarray, Optional[int]]:
    if vectorize:
        X_train_vec = sym_matrix_to_vec(X_train, discard_diagonal=True)
        X_test_vec = sym_matrix_to_vec(X_test, discard_diagonal=True)
    else:
        X_train_vec = X_train
        X_test_vec = X_test

    if scale:
        scaler = StandardScaler()
        X_train_vec = scaler.fit_transform(X_train_vec)
        X_test_vec = scaler.transform(X_test_vec)

    if pca_components is None or pca_components == 0:
        return X_train_vec, X_test_vec, None

    pca = PCA(n_components=pca_components, random_state=random_state)
    X_train_pca = pca.fit_transform(X_train_vec)
    X_test_pca = pca.transform(X_test_vec)
    return X_train_pca, X_test_pca, int(pca.n_components_)


def _get_positive_proba(estimator, X: np.ndarray) -> Optional[np.ndarray]:
    predict_proba = getattr(estimator, "predict_proba", None)
    if predict_proba is None:
        return None
    proba = predict_proba(X)
    proba = np.asarray(proba)
    if proba.ndim != 2 or proba.shape[1] != 2:
        raise ValueError(f"Expected predict_proba -> (n_samples, 2), got {proba.shape}")
    return proba[:, 1]


def _get_continuous_score(estimator, X: np.ndarray) -> Optional[np.ndarray]:
    """
    Return a continuous score for ROC AUC.
    Prefer decision_function when available; fall back to P(y=1) if not.
    """
    decision_function = getattr(estimator, "decision_function", None)
    if decision_function is not None:
        scores = np.asarray(decision_function(X))
        if scores.ndim != 1:
            scores = scores.reshape(-1)
        return scores
    return _get_positive_proba(estimator, X)


def prob_true_class(y_true: np.ndarray, p_positive: np.ndarray) -> np.ndarray:
    """
    Per-sample probability assigned to the true class (binary y in {0,1}).
    """
    y_true = np.asarray(y_true).astype(int)
    p_positive = np.asarray(p_positive, dtype=float)
    if y_true.shape[0] != p_positive.shape[0]:
        raise ValueError("y_true and p_positive must have the same length")
    return np.where(y_true == 1, p_positive, 1.0 - p_positive)


def sign_flip_randomization_test(
    diff: np.ndarray,
    *,
    n_permutations: int = 10000,
    random_state: int = 42,
    alternative: Alternative = "two-sided",
) -> Dict[str, Any]:
    """
    Sign-flip randomization test for mean(diff).
    """
    diff = np.asarray(diff, dtype=float).reshape(-1)
    if diff.shape[0] == 0:
        raise ValueError("diff must contain at least one element")

    observed = float(np.mean(diff))

    rng = np.random.RandomState(random_state)
    signs = rng.choice([-1.0, 1.0], size=(n_permutations, diff.shape[0]))
    null = (signs * diff).mean(axis=1)

    if alternative == "greater":
        p_value = (np.sum(null >= observed) + 1) / (n_permutations + 1)
    elif alternative == "less":
        p_value = (np.sum(null <= observed) + 1) / (n_permutations + 1)
    elif alternative == "two-sided":
        p_value = (np.sum(np.abs(null) >= abs(observed)) + 1) / (n_permutations + 1)
    else:
        raise ValueError(f"Unknown alternative: {alternative}")

    return {
        "observed_delta": observed,
        "p_value": float(p_value),
        "null_mean": float(np.mean(null)),
        "null_std": float(np.std(null)),
        "n_permutations": int(n_permutations),
        "n_units": int(diff.shape[0]),
        "alternative": alternative,
    }


def paired_randomization_test(
    scores_a: np.ndarray,
    scores_b: np.ndarray,
    *,
    n_permutations: int = 10000,
    random_state: int = 42,
    alternative: Alternative = "two-sided",
    groups: Optional[np.ndarray] = None,
) -> Dict[str, Any]:
    """
    Paired randomization (swap / sign-flip) test for mean(scores_a - scores_b).

    Under H0, pipelines A and B are exchangeable per sample, i.e. each paired
    difference can flip sign with probability 0.5.
    """
    scores_a = np.asarray(scores_a, dtype=float)
    scores_b = np.asarray(scores_b, dtype=float)
    if scores_a.shape != scores_b.shape:
        raise ValueError("scores_a and scores_b must have the same shape")

    diff = scores_a - scores_b
    if groups is not None:
        groups = np.asarray(groups)
        if groups.shape[0] != diff.shape[0]:
            raise ValueError("groups must have the same length as scores")
        unique_groups, inv = np.unique(groups, return_inverse=True)
        diff_by_group = np.zeros(unique_groups.shape[0], dtype=float)
        for gi in range(unique_groups.shape[0]):
            diff_by_group[gi] = float(np.mean(diff[inv == gi]))
        diff = diff_by_group

    result = sign_flip_randomization_test(
        diff,
        n_permutations=n_permutations,
        random_state=random_state,
        alternative=alternative,
    )
    result["grouped"] = groups is not None
    return result


def paired_randomization_test_prob_true(
    y_true: np.ndarray,
    p_positive_a: np.ndarray,
    p_positive_b: np.ndarray,
    *,
    n_permutations: int = 10000,
    random_state: int = 42,
    alternative: Alternative = "two-sided",
    groups: Optional[np.ndarray] = None,
) -> Dict[str, Any]:
    """
    Paired pipeline comparison using per-sample probability of the true class.

    Uses score_i = P_hat(y_i | x_i) for each pipeline, then performs a paired
    randomization (swap) test on the mean difference.
    """
    scores_a = prob_true_class(y_true, p_positive_a)
    scores_b = prob_true_class(y_true, p_positive_b)
    return paired_randomization_test(
        scores_a,
        scores_b,
        n_permutations=n_permutations,
        random_state=random_state,
        alternative=alternative,
        groups=groups,
    )


def permutation_test(
    X_train: np.ndarray,
    y_train: np.ndarray,
    X_test: np.ndarray,
    y_test: np.ndarray,
    n_permutations: int = 1000,
    pca_components: float = 0.95,
    random_state: int = 42,
    vectorize: bool = True,
    model: ModelName = "logreg",
    model_params: Optional[Dict[str, Any]] = None,
    scale: bool = False,
) -> dict:
    """
    Permutation test for classification accuracy.

    Tests whether the observed classification accuracy is significantly
    better than chance by comparing to a null distribution generated
    by randomly permuting training labels.

    The null hypothesis is that there is no association between the
    features and labels. Under this hypothesis, permuting labels should
    not affect classification performance (beyond random variation).

    Parameters
    ----------
    X_train : np.ndarray
        Training data. If vectorize=True, expects FC matrices (n, rois, rois).
        Otherwise expects already vectorized data (n, features).
    y_train : np.ndarray
        Training labels
    X_test : np.ndarray
        Test data (same format as X_train)
    y_test : np.ndarray
        Test labels
    n_permutations : int
        Number of permutations for null distribution (default: 1000)
    pca_components : float
        PCA variance to retain (0-1) or number of components
    random_state : int
        Random seed for reproducibility
    vectorize : bool
        If True, vectorize FC matrices using sym_matrix_to_vec

    Returns
    -------
    dict with keys:
        - observed_acc: float, the actual test accuracy
        - p_value: float, proportion of permuted accuracies >= observed
        - null_distribution: np.ndarray, permuted accuracies
        - mean_null: float, mean of null distribution
        - std_null: float, std of null distribution
    """
    rng = np.random.RandomState(random_state)

    X_train_feat, X_test_feat, _ = preprocess_features(
        X_train,
        X_test,
        pca_components=pca_components,
        random_state=random_state,
        scale=scale,
        vectorize=vectorize,
    )

    # Get observed accuracy with original labels
    clf = build_estimator(model, random_state=random_state, params=model_params)
    clf.fit(X_train_feat, y_train)
    y_pred = clf.predict(X_test_feat)
    observed_acc = accuracy_score(y_test, y_pred)

    # Generate null distribution by permuting training labels
    null_distribution = np.zeros(n_permutations)

    for i in range(n_permutations):
        # Permute training labels only (test labels stay fixed)
        y_train_perm = rng.permutation(y_train)

        # Train on permuted labels
        clf_perm = clone(clf)
        clf_perm.fit(X_train_feat, y_train_perm)

        # Evaluate on original test labels
        y_pred_perm = clf_perm.predict(X_test_feat)
        null_distribution[i] = accuracy_score(y_test, y_pred_perm)

    # Compute p-value (proportion of permuted >= observed)
    # Add 1 to numerator and denominator for conservative estimate
    p_value = (np.sum(null_distribution >= observed_acc) + 1) / (n_permutations + 1)

    return {
        'observed_acc': observed_acc,
        'p_value': p_value,
        'null_distribution': null_distribution,
        'mean_null': np.mean(null_distribution),
        'std_null': np.std(null_distribution)
    }


def permutation_test_precomputed(
    X_train_features: np.ndarray,
    y_train: np.ndarray,
    X_test_features: np.ndarray,
    y_test: np.ndarray,
    observed_acc: float,
    n_permutations: int = 1000,
    random_state: int = 42,
    model: ModelName = "logreg",
    model_params: Optional[Dict[str, Any]] = None,
) -> dict:
    """
    Permutation test using pre-computed PCA features.

    This is more efficient when PCA is already computed (e.g., in the main
    classification pipeline) and we want to add permutation testing.

    Parameters
    ----------
    X_train_pca : np.ndarray
        PCA-transformed training features
    y_train : np.ndarray
        Training labels
    X_test_pca : np.ndarray
        PCA-transformed test features
    y_test : np.ndarray
        Test labels
    observed_acc : float
        Already computed test accuracy (to avoid recomputation)
    n_permutations : int
        Number of permutations
    random_state : int
        Random seed

    Returns
    -------
    dict with p_value, null_distribution, mean_null, std_null
    """
    rng = np.random.RandomState(random_state)

    null_distribution = np.zeros(n_permutations)
    base_clf = build_estimator(model, random_state=random_state, params=model_params)

    for i in range(n_permutations):
        y_train_perm = rng.permutation(y_train)

        clf_perm = clone(base_clf)
        clf_perm.fit(X_train_features, y_train_perm)

        y_pred_perm = clf_perm.predict(X_test_features)
        null_distribution[i] = accuracy_score(y_test, y_pred_perm)

    p_value = (np.sum(null_distribution >= observed_acc) + 1) / (n_permutations + 1)

    return {
        'observed_acc': observed_acc,
        'p_value': p_value,
        'null_distribution': null_distribution,
        'mean_null': np.mean(null_distribution),
        'std_null': np.std(null_distribution)
    }


def run_classification_with_permutation(
    X_train: np.ndarray,
    y_train: np.ndarray,
    X_test: np.ndarray,
    y_test: np.ndarray,
    pca_components: float = 0.95,
    random_state: int = 42,
    n_permutations: int = 1000,
    model: ModelName = "logreg",
    model_params: Optional[Dict[str, Any]] = None,
    scale: Optional[bool] = None,
    return_probabilities: bool = False,
    return_test_outputs: bool = False,
    vectorize: bool = True,
) -> dict:
    """
    Run classification with integrated permutation testing.

    This function performs both classification and permutation testing
    in a single call, reusing PCA computation for efficiency.

    Parameters
    ----------
    X_train : np.ndarray
        Training FC matrices (n_train, n_rois, n_rois) if vectorize=True,
        or already vectorized features (n_train, n_features) if vectorize=False.
    y_train : np.ndarray
        Training labels
    X_test : np.ndarray
        Test FC matrices (n_test, n_rois, n_rois) if vectorize=True,
        or already vectorized features (n_test, n_features) if vectorize=False.
    y_test : np.ndarray
        Test labels
    pca_components : float
        PCA variance to retain
    random_state : int
        Random seed
    n_permutations : int
        Number of permutations (0 to skip permutation test)
    vectorize : bool, default=True
        If True, vectorize FC matrices using sym_matrix_to_vec.
        If False, assume input is already vectorized (2D feature arrays).

    Returns
    -------
    dict with keys:
        - train_acc: training accuracy
        - test_acc: test accuracy
        - n_pca_components: number of PCA components
        - p_value: permutation test p-value (None if n_permutations=0)
        - mean_null: mean of null distribution (None if n_permutations=0)
        - std_null: std of null distribution (None if n_permutations=0)
    """
    if scale is None:
        scale = model == "svm_rbf"

    X_train_feat, X_test_feat, n_pca_components = preprocess_features(
        X_train,
        X_test,
        pca_components=pca_components,
        random_state=random_state,
        scale=scale,
        vectorize=vectorize,
    )

    # Train classifier
    clf = build_estimator(model, random_state=random_state, params=model_params)
    clf.fit(X_train_feat, y_train)

    # Predictions
    y_train_pred = clf.predict(X_train_feat)
    y_test_pred = clf.predict(X_test_feat)

    train_acc = accuracy_score(y_train, y_train_pred)
    test_acc = accuracy_score(y_test, y_test_pred)

    train_score = _get_continuous_score(clf, X_train_feat)
    test_score = _get_continuous_score(clf, X_test_feat)
    train_auc = None
    test_auc = None
    if train_score is not None:
        try:
            train_auc = float(roc_auc_score(y_train, train_score))
        except ValueError:
            train_auc = None
    if test_score is not None:
        try:
            test_auc = float(roc_auc_score(y_test, test_score))
        except ValueError:
            test_auc = None

    train_p = _get_positive_proba(clf, X_train_feat)
    test_p = _get_positive_proba(clf, X_test_feat)
    train_brier = float(brier_score_loss(y_train, train_p)) if train_p is not None else None
    test_brier = float(brier_score_loss(y_test, test_p)) if test_p is not None else None

    result = {
        'model': model,
        'model_params': model_params or {},
        'scale': bool(scale),
        'train_acc': train_acc,
        'test_acc': test_acc,
        'train_auc': train_auc,
        'test_auc': test_auc,
        'train_brier': train_brier,
        'test_brier': test_brier,
        'n_pca_components': n_pca_components,
        'p_value': None,
        'mean_null': None,
        'std_null': None
    }
    if return_probabilities:
        result["test_p_positive"] = test_p.tolist() if test_p is not None else None
    if return_test_outputs:
        result["test_y_pred"] = np.asarray(y_test_pred).astype(int).tolist()
        result["test_score"] = test_score.tolist() if test_score is not None else None
        result["test_p_positive"] = test_p.tolist() if test_p is not None else None

    # Run permutation test if requested
    if n_permutations > 0:
        perm_result = permutation_test_precomputed(
            X_train_feat, y_train,
            X_test_feat, y_test,
            observed_acc=test_acc,
            n_permutations=n_permutations,
            random_state=random_state,
            model=model,
            model_params=model_params,
        )
        result['p_value'] = perm_result['p_value']
        result['mean_null'] = perm_result['mean_null']
        result['std_null'] = perm_result['std_null']

    return result


# =============================================================================
# Pipeline comparison utilities (cross-site)
# =============================================================================

PIPELINE_COLUMNS = (
    "atlas",
    "fc_type",
    "strategy",
    "gsr",
    "model",
    "model_params",
)


def infer_pipeline_columns(df: pd.DataFrame, *, include_sites: bool = False) -> list[str]:
    cols = [col for col in PIPELINE_COLUMNS if col in df.columns]
    if include_sites:
        for col in ("train_site", "test_site"):
            if col in df.columns and col not in cols:
                cols.insert(0, col)
    return cols


def filter_test_outputs(
    df: pd.DataFrame,
    *,
    train_site: Optional[str] = None,
    test_site: Optional[str] = None,
) -> pd.DataFrame:
    out = df
    if train_site is not None:
        if "train_site" not in out.columns:
            raise ValueError("train_site filter requested but column missing")
        out = out[out["train_site"] == train_site]
    if test_site is not None:
        if "test_site" not in out.columns:
            raise ValueError("test_site filter requested but column missing")
        out = out[out["test_site"] == test_site]
    return out


def compute_sample_loss(
    y_true: np.ndarray,
    *,
    y_pred: Optional[np.ndarray] = None,
    p_positive: Optional[np.ndarray] = None,
    metric: str = "log_loss",
    eps: float = 1e-15,
) -> np.ndarray:
    """
    Compute per-sample losses (lower is better).

    Note: metric="accuracy"/"acc" returns classification error (1 - accuracy).
    """
    metric = metric.lower()
    y_true = np.asarray(y_true, dtype=int).reshape(-1)

    if metric in {"log_loss", "logloss", "cross_entropy"}:
        if p_positive is None:
            raise ValueError("log_loss requires p_positive")
        p = np.asarray(p_positive, dtype=float).reshape(-1)
        if p.shape[0] != y_true.shape[0]:
            raise ValueError("p_positive and y_true must have the same length")
        if not np.isfinite(p).all():
            raise ValueError("p_positive must be finite for log_loss")
        p = np.clip(p, eps, 1.0 - eps)
        return -(y_true * np.log(p) + (1 - y_true) * np.log(1 - p))

    if metric == "brier":
        if p_positive is None:
            raise ValueError("brier requires p_positive")
        p = np.asarray(p_positive, dtype=float).reshape(-1)
        if p.shape[0] != y_true.shape[0]:
            raise ValueError("p_positive and y_true must have the same length")
        if not np.isfinite(p).all():
            raise ValueError("p_positive must be finite for brier")
        return (p - y_true) ** 2

    if metric in {"error", "accuracy", "acc"}:
        if y_pred is None:
            raise ValueError("accuracy/error requires y_pred")
        y_pred = np.asarray(y_pred, dtype=int).reshape(-1)
        if y_pred.shape[0] != y_true.shape[0]:
            raise ValueError("y_pred and y_true must have the same length")
        correct = (y_pred == y_true).astype(float)
        if metric in {"accuracy", "acc"}:
            return 1.0 - correct
        return 1.0 - correct

    raise ValueError(f"Unknown metric: {metric}")


def _aggregate_subject_losses(
    df: pd.DataFrame,
    *,
    metric: str,
    subject_col: str,
    pipeline_cols: Iterable[str],
) -> pd.DataFrame:
    loss = compute_sample_loss(
        df["y_true"].to_numpy(),
        y_pred=df.get("y_pred"),
        p_positive=df.get("p_positive"),
        metric=metric,
    )
    cols = list(pipeline_cols) + [subject_col]
    tmp = df[cols].copy()
    tmp["loss"] = loss
    return tmp.groupby(cols, as_index=False)["loss"].mean()


def prepare_factor_pair_differences(
    test_outputs: pd.DataFrame,
    *,
    factor: str,
    level_a: Any,
    level_b: Any,
    metric: str = "log_loss",
    pipeline_cols: Optional[Iterable[str]] = None,
    subject_col: str = "test_subject",
    train_site: Optional[str] = None,
    test_site: Optional[str] = None,
) -> pd.DataFrame:
    """
    Build per-subject paired differences for a factor-level comparison.

    Returns a DataFrame with columns:
    - match columns (all pipeline columns except `factor`)
    - subject_col
    - loss_a, loss_b
    - d = loss_b - loss_a  (positive means level_a is better)
    """
    df = filter_test_outputs(test_outputs, train_site=train_site, test_site=test_site)
    if pipeline_cols is None:
        pipeline_cols = infer_pipeline_columns(df, include_sites=train_site is None or test_site is None)

    pipeline_cols = list(pipeline_cols)
    if factor not in pipeline_cols:
        raise ValueError(f"Factor '{factor}' not found in pipeline columns: {pipeline_cols}")

    df = df[df[factor].isin([level_a, level_b])]
    if df.empty:
        raise ValueError("No rows matched the requested factor levels")

    match_cols = [col for col in pipeline_cols if col != factor]
    sample_cols = [subject_col]
    if "y_true" in df.columns:
        sample_cols.append("y_true")

    subject_sets = {}
    for key, sub in df.groupby(match_cols + [factor], dropna=False):
        if not isinstance(key, tuple):
            key = (key,)
        match_key = key[:-1]
        level = key[-1]
        sample_set = frozenset(
            sub[sample_cols].drop_duplicates().itertuples(index=False, name=None)
        )
        subject_sets.setdefault(match_key, {})[level] = sample_set

    mismatches = []
    for match_key, levels in subject_sets.items():
        if level_a in levels and level_b in levels and levels[level_a] != levels[level_b]:
            mismatches.append(
                (match_key, len(levels[level_a]), len(levels[level_b]))
            )
    if mismatches:
        example = mismatches[0]
        if match_cols:
            match_desc = ", ".join(
                f"{col}={val}" for col, val in zip(match_cols, example[0])
            )
        else:
            match_desc = "<all pipelines>"
        raise ValueError(
            "Mismatched test samples between factor levels for matched pipelines. "
            f"Example match: {match_desc} "
            f"(n_samples {level_a}={example[1]}, {level_b}={example[2]})."
        )

    subject_losses = _aggregate_subject_losses(
        df,
        metric=metric,
        subject_col=subject_col,
        pipeline_cols=pipeline_cols,
    )
    pivot = subject_losses.pivot_table(
        index=match_cols + [subject_col],
        columns=factor,
        values="loss",
        aggfunc="mean",
    )
    if level_a not in pivot.columns or level_b not in pivot.columns:
        raise ValueError("Missing one or both factor levels after pivoting")

    pairs = pivot[[level_a, level_b]].dropna().reset_index()
    pairs = pairs.rename(columns={level_a: "loss_a", level_b: "loss_b"})
    pairs["d"] = pairs["loss_b"] - pairs["loss_a"]
    return pairs


def bootstrap_mean_ci(
    values: np.ndarray,
    *,
    n_bootstrap: int = 10000,
    random_state: int = 42,
    alpha: float = 0.05,
) -> Dict[str, Any]:
    values = np.asarray(values, dtype=float).reshape(-1)
    if values.shape[0] == 0:
        raise ValueError("values must contain at least one element")
    rng = np.random.RandomState(random_state)
    n = values.shape[0]
    boot = np.empty(n_bootstrap, dtype=float)
    for i in range(n_bootstrap):
        boot[i] = np.mean(rng.choice(values, size=n, replace=True))
    lower = float(np.quantile(boot, alpha / 2.0))
    upper = float(np.quantile(boot, 1.0 - alpha / 2.0))
    return {
        "bootstrap_mean": float(np.mean(boot)),
        "ci_lower": lower,
        "ci_upper": upper,
        "n_bootstrap": int(n_bootstrap),
        "alpha": float(alpha),
    }


def factor_level_randomization_test(
    test_outputs: pd.DataFrame,
    *,
    factor: str,
    level_a: Any,
    level_b: Any,
    metric: str = "log_loss",
    pipeline_cols: Optional[Iterable[str]] = None,
    subject_col: str = "test_subject",
    train_site: Optional[str] = None,
    test_site: Optional[str] = None,
    n_permutations: int = 10000,
    random_state: int = 42,
    alternative: Alternative = "two-sided",
    n_bootstrap: int = 0,
) -> Dict[str, Any]:
    """
    Global factor-level comparison using subject-level sign-flip randomization.
    """
    pairs = prepare_factor_pair_differences(
        test_outputs,
        factor=factor,
        level_a=level_a,
        level_b=level_b,
        metric=metric,
        pipeline_cols=pipeline_cols,
        subject_col=subject_col,
        train_site=train_site,
        test_site=test_site,
    )
    subject_effects = pairs.groupby(subject_col, as_index=True)["d"].mean()
    result = sign_flip_randomization_test(
        subject_effects.to_numpy(),
        n_permutations=n_permutations,
        random_state=random_state,
        alternative=alternative,
    )
    match_cols = [col for col in pairs.columns if col not in {subject_col, "loss_a", "loss_b", "d"}]
    result.update(
        {
            "factor": factor,
            "level_a": level_a,
            "level_b": level_b,
            "metric": metric,
            "n_subjects": int(subject_effects.shape[0]),
            "n_pairs": int(pairs[match_cols].drop_duplicates().shape[0]),
            "train_site": train_site,
            "test_site": test_site,
        }
    )
    if n_bootstrap > 0:
        result.update(
            bootstrap_mean_ci(
                subject_effects.to_numpy(),
                n_bootstrap=n_bootstrap,
                random_state=random_state,
            )
        )
    return result


def load_test_outputs(path: str) -> pd.DataFrame:
    return pd.read_csv(path)


def load_pipeline_abbreviations(path: str) -> pd.DataFrame:
    return pd.read_csv(path)


def resolve_pipeline_spec(
    pipeline_spec: Any,
    *,
    abbrev_df: Optional[pd.DataFrame] = None,
) -> Dict[str, Any]:
    if isinstance(pipeline_spec, pd.Series):
        spec = pipeline_spec.to_dict()
    elif isinstance(pipeline_spec, dict):
        spec = dict(pipeline_spec)
    elif isinstance(pipeline_spec, str):
        if abbrev_df is None:
            raise ValueError("abbrev_df is required when pipeline_spec is an abbrev string")
        match = abbrev_df[abbrev_df["abbrev"] == pipeline_spec]
        if match.shape[0] != 1:
            raise ValueError(f"Expected one match for abbrev '{pipeline_spec}', found {match.shape[0]}")
        spec = match.iloc[0].to_dict()
    else:
        raise ValueError("pipeline_spec must be a dict, Series, or abbrev string")

    spec.pop("abbrev", None)
    spec.pop("spec", None)

    if "model_params" in spec and isinstance(spec["model_params"], dict):
        spec["model_params"] = json.dumps(spec["model_params"], sort_keys=True)

    return spec


def filter_pipeline_rows(
    test_outputs: pd.DataFrame,
    pipeline_spec: Any,
    *,
    abbrev_df: Optional[pd.DataFrame] = None,
) -> pd.DataFrame:
    spec = resolve_pipeline_spec(pipeline_spec, abbrev_df=abbrev_df)
    missing_cols = [col for col in spec.keys() if col not in test_outputs.columns]
    if missing_cols:
        raise ValueError(f"Pipeline spec contains unknown columns: {missing_cols}")
    if not spec:
        raise ValueError("Pipeline spec is empty after normalization")

    mask = np.ones(len(test_outputs), dtype=bool)
    for key, value in spec.items():
        mask &= test_outputs[key] == value
    subset = test_outputs[mask]
    if subset.empty:
        raise ValueError("No rows matched the pipeline specification")
    return subset


def merge_pipeline_outputs(
    test_outputs: pd.DataFrame,
    pipeline_a: Any,
    pipeline_b: Any,
    *,
    abbrev_df: Optional[pd.DataFrame] = None,
    on_cols: Optional[Iterable[str]] = None,
) -> pd.DataFrame:
    df_a = filter_pipeline_rows(test_outputs, pipeline_a, abbrev_df=abbrev_df)
    df_b = filter_pipeline_rows(test_outputs, pipeline_b, abbrev_df=abbrev_df)

    if on_cols is None:
        on_cols = ["sample_index", "test_subject", "y_true"]
    on_cols = list(on_cols)

    merged = df_a.merge(df_b, on=on_cols, suffixes=("_a", "_b"), how="inner")
    if merged.empty:
        raise ValueError("No overlapping samples between pipeline A and B")
    if len(merged) != len(df_a) or len(merged) != len(df_b):
        raise ValueError(
            "Pipeline A and B do not share identical test samples "
            f"(n_a={len(df_a)}, n_b={len(df_b)}, n_overlap={len(merged)})."
        )

    return merged.sort_values(on_cols).reset_index(drop=True)


def mcnemar_exact_test(
    y_true: np.ndarray,
    y_pred_a: np.ndarray,
    y_pred_b: np.ndarray,
    *,
    alternative: Alternative = "two-sided",
) -> Dict[str, Any]:
    y_true = np.asarray(y_true, dtype=int).reshape(-1)
    y_pred_a = np.asarray(y_pred_a, dtype=int).reshape(-1)
    y_pred_b = np.asarray(y_pred_b, dtype=int).reshape(-1)
    if y_true.shape != y_pred_a.shape or y_true.shape != y_pred_b.shape:
        raise ValueError("y_true and y_pred arrays must have the same shape")

    a_correct = y_pred_a == y_true
    b_correct = y_pred_b == y_true
    n10 = int(np.sum(a_correct & ~b_correct))
    n01 = int(np.sum(~a_correct & b_correct))
    n = n10 + n01

    if n == 0:
        p_value = 1.0
    else:
        try:
            p_value = stats.binomtest(n10, n, 0.5, alternative=alternative).pvalue
        except AttributeError:
            p_value = stats.binom_test(n10, n, 0.5, alternative=alternative)

    return {
        "acc_a": float(np.mean(a_correct)),
        "acc_b": float(np.mean(b_correct)),
        "delta_acc": float(np.mean(a_correct) - np.mean(b_correct)),
        "n10": n10,
        "n01": n01,
        "n": n,
        "p_value": float(p_value),
        "alternative": alternative,
    }


def _compute_midrank(x: np.ndarray) -> np.ndarray:
    x = np.asarray(x, dtype=float)
    order = np.argsort(x)
    x_sorted = x[order]
    n = x_sorted.shape[0]
    midranks = np.zeros(n, dtype=float)
    i = 0
    while i < n:
        j = i
        while j < n and x_sorted[j] == x_sorted[i]:
            j += 1
        midranks[i:j] = 0.5 * (i + j - 1) + 1
        i = j
    out = np.empty(n, dtype=float)
    out[order] = midranks
    return out


def _fast_delong(predictions_sorted_transposed: np.ndarray, label_1_count: int):
    m = int(label_1_count)
    if m <= 0:
        raise ValueError("label_1_count must be > 0")
    n = predictions_sorted_transposed.shape[1] - m
    if n <= 0:
        raise ValueError("Need at least one negative example")

    positive_examples = predictions_sorted_transposed[:, :m]
    negative_examples = predictions_sorted_transposed[:, m:]

    tx = np.array([_compute_midrank(x) for x in positive_examples])
    ty = np.array([_compute_midrank(x) for x in negative_examples])
    tz = np.array([_compute_midrank(x) for x in predictions_sorted_transposed])

    aucs = (tz[:, :m].sum(axis=1) - m * (m + 1) / 2.0) / (m * n)
    v01 = (tz[:, :m] - tx) / n
    v10 = 1.0 - (tz[:, m:] - ty) / m
    sx = np.cov(v01, bias=False)
    sy = np.cov(v10, bias=False)
    delong_cov = sx / m + sy / n

    return aucs, delong_cov


def delong_roc_test(
    y_true: np.ndarray,
    scores_a: np.ndarray,
    scores_b: np.ndarray,
    *,
    alternative: Alternative = "two-sided",
) -> Dict[str, Any]:
    y_true = np.asarray(y_true, dtype=int).reshape(-1)
    scores_a = np.asarray(scores_a, dtype=float).reshape(-1)
    scores_b = np.asarray(scores_b, dtype=float).reshape(-1)
    if y_true.shape != scores_a.shape or y_true.shape != scores_b.shape:
        raise ValueError("y_true and score arrays must have the same shape")

    if np.unique(y_true).size != 2:
        raise ValueError("y_true must contain both classes for ROC AUC")
    if not np.isfinite(scores_a).all() or not np.isfinite(scores_b).all():
        raise ValueError("Scores must be finite for DeLong test")

    order = np.argsort(-y_true)
    predictions = np.vstack([scores_a, scores_b])
    preds_sorted = predictions[:, order]
    label_1_count = int(np.sum(y_true))

    aucs, cov = _fast_delong(preds_sorted, label_1_count)
    delta = float(aucs[0] - aucs[1])
    var = float(cov[0, 0] + cov[1, 1] - 2.0 * cov[0, 1])
    if not np.isfinite(var) or var <= 0:
        return {
            "auc_a": float(aucs[0]),
            "auc_b": float(aucs[1]),
            "delta_auc": delta,
            "variance": var,
            "z_stat": float("nan"),
            "p_value": float("nan"),
            "alternative": alternative,
        }

    z_stat = delta / np.sqrt(var)
    if alternative == "greater":
        p_value = 1.0 - stats.norm.cdf(z_stat)
    elif alternative == "less":
        p_value = stats.norm.cdf(z_stat)
    elif alternative == "two-sided":
        p_value = 2.0 * (1.0 - stats.norm.cdf(abs(z_stat)))
    else:
        raise ValueError(f"Unknown alternative: {alternative}")

    return {
        "auc_a": float(aucs[0]),
        "auc_b": float(aucs[1]),
        "delta_auc": delta,
        "variance": var,
        "z_stat": float(z_stat),
        "p_value": float(p_value),
        "alternative": alternative,
    }


def compare_pipelines(
    test_outputs: pd.DataFrame,
    pipeline_a: Any,
    pipeline_b: Any,
    *,
    abbrev_df: Optional[pd.DataFrame] = None,
    alternative: Alternative = "two-sided",
) -> Dict[str, Any]:
    """
    Compare two pipelines (A vs B) with Exact McNemar and DeLong tests.
    """
    merged = merge_pipeline_outputs(
        test_outputs,
        pipeline_a,
        pipeline_b,
        abbrev_df=abbrev_df,
    )

    mcnemar = mcnemar_exact_test(
        merged["y_true"],
        merged["y_pred_a"],
        merged["y_pred_b"],
        alternative=alternative,
    )

    score_a = merged["y_score_a"]
    score_b = merged["y_score_b"]
    if score_a.isna().all() or score_b.isna().all():
        if "p_positive_a" not in merged.columns or "p_positive_b" not in merged.columns:
            raise ValueError("No continuous scores available for DeLong test")
        score_a = merged["p_positive_a"]
        score_b = merged["p_positive_b"]

    delong = delong_roc_test(
        merged["y_true"],
        score_a,
        score_b,
        alternative=alternative,
    )

    return {
        "n_samples": int(merged.shape[0]),
        "mcnemar": mcnemar,
        "delong": delong,
    }
